{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ce41c2-3eab-4e73-b168-9bb81f1fa1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/joaquinoldan/Documents/MASTER_AI/4-Tesis/A-Github/lalossSQL/Entrega/OpenAI', '/Users/joaquinoldan/opt/anaconda3/envs/langchain/lib/python310.zip', '/Users/joaquinoldan/opt/anaconda3/envs/langchain/lib/python3.10', '/Users/joaquinoldan/opt/anaconda3/envs/langchain/lib/python3.10/lib-dynload', '', '/Users/joaquinoldan/opt/anaconda3/envs/langchain/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "from langchain.llms  import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bf41b5-493d-4f3c-851b-7697ec3d0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = 'ADD_YOUR_API_KEY_HERE'\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "model_prices = [{'model':'gpt-4-1106-preview','input_price':10,'output_price':30},\n",
    "                {'model':'gpt-3.5-turbo-0125','input_price':0.5,'output_price':1.5},\n",
    "                {'model':'davinci-002','input_price':2,'output_price':2},\n",
    "                {'model':'babbage-002','input_price':0.4,'output_price':0.4}]\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "#change model name to use different model\n",
    "model_name = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "#For davinci of babage use this generation function\n",
    "def base_model_generation(prompt):\n",
    "  response = openai.completions.create(\n",
    "    model = model_name ,\n",
    "    prompt=prompt,\n",
    "    n = 1,\n",
    "    stream = False,\n",
    "    temperature=0.0,\n",
    "    max_tokens=600,\n",
    "    top_p = 1.0,\n",
    "      \n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    stop = [\"SQL\"]\n",
    "  ) \n",
    "  return response.choices[0].text, response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "\n",
    "#For GPT3.5 and GPT4 use this generation function\n",
    "def GPT_generation(system_prompt, user_prompt):\n",
    "  response = client.chat.completions.create(\n",
    "    model= model_name ,\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt},{\"role\": \"user\", \"content\": user_prompt}],\n",
    "    n = 1,\n",
    "    stream = False,\n",
    "    temperature=0.0,\n",
    "    max_tokens=600,\n",
    "    top_p = 1.0,\n",
    "    \n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "  )\n",
    "  return response.choices[0].message.content, response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928a04c6-2afd-4d3e-924b-e6e61c656f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_linking_prompt = '''\n",
    "Schema:\n",
    "Table advisor, columns = [*,s_ID,i_ID]\n",
    "Table classroom, columns = [*,building,room_number,capacity]\n",
    "Table course, columns = [*,course_id,title,dept_name,credits]\n",
    "Table department, columns = [*,dept_name,building,budget]\n",
    "Table instructor, columns = [*,ID,name,dept_name,salary]\n",
    "Table prereq, columns = [*,course_id,prereq_id]\n",
    "Table section, columns = [*,course_id,sec_id,semester,year,building,room_number,time_slot_id]\n",
    "Table student, columns = [*,ID,name,dept_name,tot_cred]\n",
    "Table takes, columns = [*,ID,course_id,sec_id,semester,year,grade]\n",
    "Table teaches, columns = [*,ID,course_id,sec_id,semester,year]\n",
    "Table time_slot, columns = [*,time_slot_id,day,start_hr,start_min,end_hr,end_min]\n",
    "Foreign_keys = [course.dept_name = department.dept_name,instructor.dept_name = department.dept_name,section.building = classroom.building,section.room_number = classroom.room_number,section.course_id = course.course_id,teaches.ID = instructor.ID,teaches.course_id = section.course_id,teaches.sec_id = section.sec_id,teaches.semester = section.semester,teaches.year = section.year,student.dept_name = department.dept_name,takes.ID = student.ID,takes.course_id = section.course_id,takes.sec_id = section.sec_id,takes.semester = section.semester,takes.year = section.year,advisor.s_ID = student.ID,advisor.i_ID = instructor.ID,prereq.prereq_id = course.course_id,prereq.course_id = course.course_id]\n",
    "Q: \"Find the buildings which have rooms with capacity more than 50.\"\n",
    "A: Let’s think step by step. In the question \"Find the buildings which have rooms with capacity more than 50.\", we are asked:\n",
    "\"the buildings which have rooms\" so we need table = [classroom] since we need the column = [classroom.capacity]\n",
    "\"rooms with capacity\" so we need column = [classroom.building]\n",
    "\"rooms with capacity\" so we need table = [classroom] since we need the column = [classroom.building]\n",
    "Based on the tables, columns, and Foreign_keys. So the Tables_required are:\n",
    "Tables_Required: ['classroom']\n",
    "\n",
    "Schema:\n",
    "Table department, columns = [*,Department_ID,Name,Creation,Ranking,Budget_in_Billions,Num_Employees]\n",
    "Table head, columns = [*,head_ID,name,born_state,age]\n",
    "Table management, columns = [*,department_ID,head_ID,temporary_acting]\n",
    "Foreign_keys = [management.head_ID = head.head_ID,management.department_ID = department.Department_ID]\n",
    "Q: \"How many heads of the departments are older than 56 ?\"\n",
    "A: Let’s think step by step. In the question \"How many heads of the departments are older than 56 ?\", we are asked:\n",
    "\"How many heads of the departments\" so we need table = [head] since we need the column = [head.*]\n",
    "\"older\" than 56 so we need table = [head] since we need the column = [head.age]\n",
    "Based on the tables, columns. So the Tables_required are:\n",
    "Tables_Required: ['head']\n",
    "\n",
    "Schema:\n",
    "Table Country, columns = [*,id,name]\n",
    "Table League, columns = [*,id,country_id,name]\n",
    "Table Player, columns = [*,id,player_api_id,player_name,player_fifa_api_id,birthday,height,weight]\n",
    "Table Player_Attributes, columns = [*,id,player_fifa_api_id,player_api_id,date,overall_rating,potential,preferred_foot,attacking_work_rate,defensive_work_rate,crossing,finishing,heading_accuracy,short_passing,volleys,dribbling,curve,free_kick_accuracy,long_passing,ball_control,acceleration,sprint_speed,agility,reactions,balance,shot_power,jumping,stamina,strength,long_shots,aggression,interceptions,positioning,vision,penalties,marking,standing_tackle,sliding_tackle,gk_diving,gk_handling,gk_kicking,gk_positioning,gk_reflexes]\n",
    "Table Team, columns = [*,id,team_api_id,team_fifa_api_id,team_long_name,team_short_name]\n",
    "Table Team_Attributes, columns = [*,id,team_fifa_api_id,team_api_id,date,buildUpPlaySpeed,buildUpPlaySpeedClass,buildUpPlayDribbling,buildUpPlayDribblingClass,buildUpPlayPassing,buildUpPlayPassingClass,buildUpPlayPositioningClass,chanceCreationPassing,chanceCreationPassingClass,chanceCreationCrossing,chanceCreationCrossingClass,chanceCreationShooting,chanceCreationShootingClass,chanceCreationPositioningClass,defencePressure,defencePressureClass,defenceAggression,defenceAggressionClass,defenceTeamWidth,defenceTeamWidthClass,defenceDefenderLineClass]\n",
    "Table sqlite_sequence, columns = [*,name,seq]\n",
    "Foreign_keys = [Player_Attributes.player_api_id = Player.player_api_id,Player_Attributes.player_fifa_api_id = Player.player_fifa_api_id,League.country_id = Country.id,Team_Attributes.team_api_id = Team.team_api_id,Team_Attributes.team_fifa_api_id = Team.team_fifa_api_id]\n",
    "Q: \"List the names of all left-footed players who have overall rating between 85 and 90.\"\n",
    "A: Let’s think step by step. In the question \"List the names of all left-footed players who have overall rating between 85 and 90.\", we are asked:\n",
    "\"names of all left-footed players\" so we need table = [Player] since we need the column = [Player.player_name,Player_Attributes.preferred_foot]\n",
    "\"players who have overall rating\" so we need table = [Player_Attributes] since we need column = [Player_Attributes.overall_rating]\n",
    "Based on the tables, columns. So the Tables_required are:\n",
    "Tables_Required: ['Player', 'Player_Attributes']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03cd9650-dc65-425d-875a-31fa700a9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SCHEMA = '/Users/joaquinoldan/Documents/MASTER_AI/4-Tesis/spider/tables.json'\n",
    "DATASET = '/Users/joaquinoldan/Documents/MASTER_AI/4-Tesis/spider/dev.json'\n",
    "OUTPUT_FILE = '/Users/joaquinoldan/Documents/MASTER_AI/4-Tesis/A -Github/SQL_GPU/results'\n",
    "\n",
    "def load_data(DATASET):\n",
    "    return pd.read_json(DATASET)\n",
    "\n",
    "import time\n",
    "\n",
    "class TimeTracker:\n",
    "    def __init__(self):\n",
    "        # Initialize start_time and stop_time to None\n",
    "        self.start_time = None\n",
    "        self.stop_time = None\n",
    "\n",
    "    def start(self):\n",
    "        # Save the system time when the tracking starts\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        # Save the system time when the tracking stops\n",
    "        self.stop_time = time.time()\n",
    "\n",
    "    def get_interval(self):\n",
    "        # Calculate the time interval in seconds\n",
    "        if self.start_time is not None and self.stop_time is not None:\n",
    "            return self.stop_time - self.start_time\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def schema_linking_prompt_maker(test_sample_text,database):\n",
    "  instruction = \"# Find the schema_links for generating SQL queries for each question based on the database schema and Foreign keys.\\n\"\n",
    "  fields = find_fields_MYSQL_like(database)\n",
    "  foreign_keys = \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  prompt = instruction + schema_linking_prompt + fields + 'Q: \"' + test_sample_text + \"\"\"\"\\nA: Let’s think step by step.\"\"\"\n",
    "  #prompt = instruction + schema_linking_prompt + fields +foreign_keys+ 'Q: \"' + test_sample_text + \"\"\"\"\\nA: Let’s think step by step.\"\"\"  \n",
    "  return prompt\n",
    "\n",
    "def find_fields_MYSQL_like(db_name):\n",
    "  df = spider_schema[spider_schema['Database name'] == db_name]\n",
    "  df = df.groupby(' Table Name')\n",
    "  output = \"\"\n",
    "  for name, group in df:\n",
    "    output += \"Table \" +name+ ', columns = ['\n",
    "    for index, row in group.iterrows():\n",
    "      output += row[\" Field Name\"]+','\n",
    "    output = output[:-1]\n",
    "    output += \"]\\n\"\n",
    "  return output\n",
    "\n",
    "def find_foreign_keys_MYSQL_like(db_name):\n",
    "  df = spider_foreign[spider_foreign['Database name'] == db_name]\n",
    "  output = \"[\"\n",
    "  for index, row in df.iterrows():\n",
    "    output += row['First Table Name'] + '.' + row['First Table Foreign Key'] + \" = \" + row['Second Table Name'] + '.' + row['Second Table Foreign Key'] + ','\n",
    "  output= output[:-1] + \"]\"\n",
    "  return output\n",
    "\n",
    "def creatiing_schema(DATASET_JSON):\n",
    "    schema_df = pd.read_json(DATASET_JSON)\n",
    "    schema_df = schema_df.drop(['column_names','table_names'], axis=1)\n",
    "    schema = []\n",
    "    f_keys = []\n",
    "    p_keys = []\n",
    "    for index, row in schema_df.iterrows():\n",
    "        tables = row['table_names_original']\n",
    "        col_names = row['column_names_original']\n",
    "        col_types = row['column_types']\n",
    "        foreign_keys = row['foreign_keys']\n",
    "        primary_keys = row['primary_keys']\n",
    "        for col, col_type in zip(col_names, col_types):\n",
    "            index, col_name = col\n",
    "            if index == -1:\n",
    "                for table in tables:\n",
    "                    schema.append([row['db_id'], table, '*', 'text'])\n",
    "            else:\n",
    "                schema.append([row['db_id'], tables[index], col_name, col_type])\n",
    "        for primary_key in primary_keys:\n",
    "            index, column = col_names[primary_key]\n",
    "            p_keys.append([row['db_id'], tables[index], column])\n",
    "        for foreign_key in foreign_keys:\n",
    "            first, second = foreign_key\n",
    "            first_index, first_column = col_names[first]\n",
    "            second_index, second_column = col_names[second]\n",
    "            f_keys.append([row['db_id'], tables[first_index], tables[second_index], first_column, second_column])\n",
    "    spider_schema = pd.DataFrame(schema, columns=['Database name', ' Table Name', ' Field Name', ' Type'])\n",
    "    spider_primary = pd.DataFrame(p_keys, columns=['Database name', 'Table Name', 'Primary Key'])\n",
    "    spider_foreign = pd.DataFrame(f_keys,\n",
    "                        columns=['Database name', 'First Table Name', 'Second Table Name', 'First Table Foreign Key',\n",
    "                                 'Second Table Foreign Key'])\n",
    "    return spider_schema,spider_primary,spider_foreign\n",
    "\n",
    "\n",
    "def creatiing_schema(DATASET_JSON):\n",
    "    schema_df = pd.read_json(DATASET_JSON)\n",
    "    schema_df = schema_df.drop(['column_names','table_names'], axis=1)\n",
    "    schema = []\n",
    "    f_keys = []\n",
    "    p_keys = []\n",
    "    for index, row in schema_df.iterrows():\n",
    "        tables = row['table_names_original']\n",
    "        col_names = row['column_names_original']\n",
    "        col_types = row['column_types']\n",
    "        foreign_keys = row['foreign_keys']\n",
    "        primary_keys = row['primary_keys']\n",
    "        for col, col_type in zip(col_names, col_types):\n",
    "            index, col_name = col\n",
    "            if index == -1:\n",
    "                for table in tables:\n",
    "                    schema.append([row['db_id'], table, '*', 'text'])\n",
    "            else:\n",
    "                schema.append([row['db_id'], tables[index], col_name, col_type])\n",
    "        for primary_key in primary_keys:\n",
    "            index, column = col_names[primary_key]\n",
    "            p_keys.append([row['db_id'], tables[index], column])\n",
    "        for foreign_key in foreign_keys:\n",
    "            first, second = foreign_key\n",
    "            first_index, first_column = col_names[first]\n",
    "            second_index, second_column = col_names[second]\n",
    "            f_keys.append([row['db_id'], tables[first_index], tables[second_index], first_column, second_column])\n",
    "    spider_schema = pd.DataFrame(schema, columns=['Database name', ' Table Name', ' Field Name', ' Type'])\n",
    "    spider_primary = pd.DataFrame(p_keys, columns=['Database name', 'Table Name', 'Primary Key'])\n",
    "    spider_foreign = pd.DataFrame(f_keys,\n",
    "                        columns=['Database name', 'First Table Name', 'Second Table Name', 'First Table Foreign Key',\n",
    "                                 'Second Table Foreign Key'])\n",
    "    return spider_schema,spider_primary,spider_foreign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a912b954-646e-4f4b-94b3-d40031acf42d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples 1034\n",
      "\n",
      "index is 0\n",
      "SELECT count(*) FROM singer\n",
      "How many singers do we have?\n",
      "('In the question \"How many singers do we have?\", we are asked:\\n\"How many singers\" so we need table = [singer] since we need the column = [singer.*]\\nBased on the tables and columns provided, the Tables_required are:\\nTables_Required: [\\'singer\\']', 1437, 60)\n",
      "\n",
      "index is 1\n",
      "SELECT count(*) FROM singer\n",
      "What is the total number of singers?\n",
      "('In the question \"What is the total number of singers?\", we are asked:\\n\"total number of singers\" so we need table = [singer] since we need the column = [singer.*]\\nBased on the tables and columns, the Table required is:\\nTable_Required: [\\'singer\\']', 1438, 62)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spider_schema,spider_primary,spider_foreign = creatiing_schema(DATASET_SCHEMA)\n",
    "    val_df = load_data(DATASET)\n",
    "    print(f\"Number of data samples {val_df.shape[0]}\")\n",
    "    CODEX = []\n",
    "    count = 0\n",
    "    for index, row in val_df.iterrows():\n",
    "        print(\"\")\n",
    "        print(f\"index is {index}\")\n",
    "        print(row['query'])\n",
    "        print(row['question'])\n",
    "        table_list = None\n",
    "        error_encountered = False\n",
    "        error=None\n",
    "        while table_list is None:\n",
    "            try:\n",
    "                prompt = schema_linking_prompt_maker(row['question'], row['db_id'])\n",
    "                #table_list = base_model_generation(prompt)\n",
    "\n",
    "                table_list = GPT_generation(\"\", prompt)\n",
    "                print(table_list)\n",
    "            except Exception as e:\n",
    "                # Handling other exceptions\n",
    "                print (e)\n",
    "                error = e\n",
    "                error_encountered = True\n",
    "                pass\n",
    "        if not error_encountered:\n",
    "            try:\n",
    "                table_list = table_list.split(\"Tables_Required: \")[1]\n",
    "            except Exception as e:\n",
    "                error = e\n",
    "                error_encountered = True\n",
    "                table_list = \"[]\"\n",
    "            \n",
    "        fields = find_fields_MYSQL_like(row['db_id'])\n",
    "        foreign_keys = \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(row['db_id']) + '\\n'\n",
    "        schema = fields + foreign_keys\n",
    "        \n",
    "        CODEX.append({'question':row['question'],'gold_query':row['query'], 'db_id': row['db_id'], 'schema':schema, 'table_list':table_list, 'error_encountered':error_encountered, 'error_type': type(error).__name__, 'error_message':str(error)})\n",
    "        if index == 1: break\n",
    "    \n",
    "#    df = pd.DataFrame(CODEX, columns=['NLQ', 'PREDICTED SQL', 'GOLD SQL', 'DATABASE'])\n",
    "#    results = df['PREDICTED SQL'].tolist()\n",
    "#    with open(OUTPUT_FILE, 'w') as f:\n",
    "#       for line in results:\n",
    "#           f.write(f\"{line}\\n\")\n",
    "\n",
    "OUTPUT_FILE_DIFF = 'all-tables-GTP3.json'\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_FILE, OUTPUT_FILE_DIFF)\n",
    "\n",
    "file_path = \"output.json\"\n",
    "\n",
    "# Save the object to a JSON file\n",
    "\n",
    "\n",
    "#with open(OUTPUT_PATH, 'w') as json_file:\n",
    "#    json.dump(CODEX, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
